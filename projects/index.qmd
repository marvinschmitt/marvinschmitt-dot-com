---
title: Projects
comments: false
toc-location: right
---

## Meta-Uncertainty in Bayesian Model Comparison{#meta-uncertainty-BMC}

<img src="/img/meta_uncertainty_banner.png" style="height: 100%; width: 100%; object-fit: contain" onclick="window.open('https://arxiv.org/abs/2210.07278', 'blank');"/>

<a href="https://proceedings.mlr.press/v206/schmitt23a.html" target="_blank">Paper (AISTATS 2023)</a> | <a href="https://github.com/marvinschmitt/MetaUncertaintyPaper" target="_blank">Code</a> | <a href="https://meta-uncertainty.github.io/" target="_blank">Project website</a> | <a href="/assets/meta_uncertainty_poster_aistats.pdf" target="_blank">Poster</a> | <a href="https://www.youtube.com/watch?v=WIigoUaqy9c" target="_blank">Presentation (15min)</a><br>

Meta-Uncertainty represents a fully probabilistic framework for quantifying the uncertainty over Bayesian posterior model probabilities (PMPs) using meta-models. Meta-models integrate simulated and observed data into a predictive distribution for new PMPs and help reduce overconfidence and estimate the PMPs in future replication studies.

----

## BayesFlow: Amortized Bayesian Workflows With Neural Networks

<img src="/img/bayesflow_overview.png" style="height: 100%; width: 100%; object-fit: contain" onclick="window.open('https://bayesflow.org', 'blank');"/>

<a href="https://arxiv.org/abs/2306.16015" target="_blank">Preprint (arXiv)</a> | <a href="https://bayesflow.org" target="_blank">Project website</a><br>

BayesFlow is a Python library for simulation-based training of established neural network architectures for amortized data compression and inference. Amortized Bayesian inference, as implemented in BayesFlow, enables users to train custom neural networks on model simulations and re-use these networks for any subsequent application of the models. Since the trained networks can perform inference almost instantaneously, the upfront neural network training is quickly amortized.


----

## JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models

<img src="/img/jana_figure_1.png" style="height: 100%; width: 100%; object-fit: contain" onclick="window.open('https://arxiv.org/abs/2302.09125', 'blank');"/>

<a href="https://arxiv.org/abs/2302.09125" target="_blank">Paper (UAI 2023)</a> | <a href="https://github.com/stefanradev93/BayesFlow" target="_blank">Python library</a><br>

JANA is a new amortized solution for intractable likelihood functions and posterior densities in Bayesian modeling. It trains three networks to learn both an approximate posterior and a surrogate model for the likelihood, enabling amortized marginal likelihood and posterior predictive estimation.

----

## Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks{#sbi-model-misspecification}

<img src="/img/model_misspecification_amortized_sbi.png" style="height: 100%; width: 100%; object-fit: contain" onclick="window.open('https://arxiv.org/abs/2112.08866', 'blank');"/>

<a href="https://arxiv.org/abs/2112.08866" target="_blank">Preprint (arXiv)</a> | <a href="https://github.com/marvinschmitt/ModelMisspecificationBF" target="_blank">Code</a> | <a href="/assets/poster_bayescomp_mms.pdf" target="_blank">Poster</a><br>
Novel neural network based architectures enable amortized Bayesian inference in settings where the likelihood function is only implicitly defined by a simulation program. But how faithful is such inference when simulations represent reality somewhat inaccurately? This paper illustrates how imposing a probabilistic structure on the latent data summary space can help to detect potentially catastrophic misspecifications during inference.
