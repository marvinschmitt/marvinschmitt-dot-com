#### Content

- [Meta-Uncertainty in Bayesian Model Comparison](#meta-uncertainty-BMC)
- [Detecting Model Misspecification in Amortized SBI](#sbi-model-misspecification)

----

#### Meta-Uncertatinty

<img src="/img/meta_uncertainty_banner.png" style="height: 100%; width: 100%; object-fit: contain" onclick="window.open('https://arxiv.org/abs/2210.07278', 'blank');"/>

[arXiv Preprint](https://arxiv.org/abs/2210.07278) | [Code](https://github.com/marvinschmitt/MetaUncertaintyPaper)<br>
Meta-Uncertainty represents a fully probabilistic framework for quantifying the uncertainty over Bayesian posterior model probabilities (PMPs) using meta-models. Meta-models integrate simulated and observed data into a predictive distribution for new PMPs and help reduce overconfidence and estimate the PMPs in future replication studies.

----

#### Detecting Model Misspecification in Amortized Simulation-Based Inference with Neural Networks{#sbi-model-misspecification}

<img src="/img/model_misspecification_amortized_sbi.png" style="height: 100%; width: 100%; object-fit: contain" onclick="window.open('https://arxiv.org/abs/2112.08866', 'blank');"/>

[arXiv Preprint](https://arxiv.org/abs/2112.08866) | [Code](https://github.com/marvinschmitt/ModelMisspecificationBF)<br>
Novel neural network based architectures enable amortized Bayesian inference in settings where the likelihood function is only implicitly defined by a simulation program. But how faithful is such inference when simulations represent reality somewhat inaccurately? This paper illustrates how imposing a probabilistic structure on the latent data summary space can help to detect potentially catastrophic misspecifications during inference.
